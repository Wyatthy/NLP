{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPtest.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1RbUKNNkIHZWn3rHfXu8z90d2s9ZDr3N9",
      "authorship_tag": "ABX9TyNTbrOa6TC13NcTk8OUY48D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wyatthy/NLP/blob/main/NLPtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uosvOgQ1Y_yP"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uqtn1IdPeKh"
      },
      "source": [
        "#import\n",
        "#from snownlp import SnowNLP\n",
        "from tensorflow import keras\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model\n",
        "import csv\n",
        "import imp\n",
        "import numpy as np\n",
        "import yaml\n",
        "import sys\n",
        "import jieba\n",
        "import re\n",
        "import gensim\n",
        "import multiprocessing\n",
        "# import keras.utils\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgNrL-_7Y4o7"
      },
      "source": [
        "# infer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "srKMa9x0Y5Vo",
        "outputId": "d67f3155-a99f-4be7-a3f8-8005195881e6"
      },
      "source": [
        "#infer \n",
        "voc_dim = 150 \n",
        "model_word=Word2Vec.load('/content/drive/MyDrive/NLP/model/Word2Vec_java.pkl')\n",
        "\n",
        "input_dim = len(model_word.wv.vocab.keys()) + 1 \n",
        "embedding_weights = np.zeros((input_dim, voc_dim)) \n",
        "w2dic={} \n",
        "for i in range(len(model_word.wv.vocab.keys())): \n",
        "  embedding_weights[i + 1, :] = model_word.wv[list(model_word.wv.vocab.keys())[i]] \n",
        "  w2dic[list(model_word.wv.vocab.keys())[i]]=i+1 \n",
        "  #print(list(model_word.wv.vocab.keys())[i])\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/NLP/model/lstm_java_total.h5')\n",
        "\n",
        "pchinese = re.compile('([\\u4e00-\\u9fa5]+)+?')\n",
        "\n",
        "label={0:\"happy\",1:\"angry\",2:\"disgust\",3:\"sad\"} \n",
        "#in_stc=[\"明天\",\"就要\",\"考试\",\"我\",\"特别\",\"紧张\",\"一点\",\"都\",\"没有\",\"复习\"] \n",
        "in_str=\"抑郁\"\n",
        "\n",
        "in_stc=''.join(pchinese.findall(in_str))\n",
        "\n",
        "in_stc=list(jieba.cut(in_stc,cut_all=True, HMM=False)) \n",
        "# s=SnowNLP(in_stc) \n",
        "# in_stc=s.words \n",
        "new_txt=[]\n",
        "data=[] \n",
        "for word in in_stc: \n",
        "  try: \n",
        "    new_txt.append(w2dic[word]) \n",
        "  except: \n",
        "    new_txt.append(0) \n",
        "    data.append(new_txt)\n",
        "\n",
        "data=sequence.pad_sequences(data, maxlen=voc_dim ) \n",
        "pre=model.predict(data)[0].tolist()\n",
        "\n",
        "print(pre) \n",
        "print(\"输入:\\n \"+in_str) \n",
        "print(\"输出:\\n \"+label[pre.index(max(pre))])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6f0705c76781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mw2dic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0membedding_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mw2dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m#print(list(model_word.wv.vocab.keys())[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75nAXNDWXzkF"
      },
      "source": [
        "# DealCSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHzdJQ1vX0mi"
      },
      "source": [
        "#dealCSV 处理数据集\n",
        "\n",
        "sourcefile = '/content/drive/MyDrive/NLP/data/simplifyweibo_4_moods.csv'\n",
        "happyfile = '/content/drive/MyDrive/NLP/data/happy.txt'\n",
        "angryfile = '/content/drive/MyDrive/NLP/data/angry.txt'\n",
        "disgustfile = '/content/drive/MyDrive/NLP/data/disgust.txt'\n",
        "sadfile = '/content/drive/MyDrive/NLP/data/sad.txt'\n",
        "a=b=c=d=0\n",
        "\n",
        "with open(sourcefile, encoding='UTF-8') as f, open(happyfile, 'w', encoding='utf-8') as fp1, open(angryfile, 'w', encoding='utf-8') as fp2, open(disgustfile, 'w', encoding='utf-8') as fp3, open(sadfile, 'w', encoding='utf-8') as fp4:\n",
        "    reader = csv.reader(f)\n",
        "    head_row = next(reader)\n",
        "    for row in reader:\n",
        "        if(row[0]=='0' and a<10000):\n",
        "            fp1.write(row[1]+'\\n')\n",
        "            a+=1\n",
        "        elif(row[0]=='1' and b<10000):\n",
        "            fp2.write(row[1]+'\\n')\n",
        "            b+=1\n",
        "        elif(row[0]=='2' and c<10000):\n",
        "            fp3.write(row[1]+'\\n')\n",
        "            c+=1\n",
        "        elif(row[0]=='3' and d<10000):\n",
        "            fp4.write(row[1]+'\\n')\n",
        "            d+=1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjJDLQfBYFxE"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGMRChPvZdm8"
      },
      "source": [
        "#dataset\n",
        "#s = SnowNLP(u'这个东西真心很赞')\n",
        "\n",
        "def clean_data(rpath,wpath):\n",
        "    pchinese = re.compile('([\\u4e00-\\u9fa5]+)+?') #搞了一个正则表达对象 避免重复转换\n",
        "    f = open(rpath,encoding=\"UTF-8\")\n",
        "    fw = open(wpath, \"w\",encoding=\"UTF-8\")\n",
        "    for line in f.readlines():\n",
        "        m = pchinese.findall(str(line))\n",
        "        if m:\n",
        "            str1 = ''.join(m)\n",
        "            str2 = str(str1)\n",
        "            fw.write(str2)\n",
        "            fw.write(\"\\n\")\n",
        "    f.close()\n",
        "    fw.close()\n",
        "\n",
        "def seg_char(sent):  #分字\n",
        "    \"\"\"\n",
        "    把句子按字分开，不破坏英文结构\n",
        "    \"\"\"\n",
        "    # 首先分割 英文 以及英文和标点\n",
        "    pattern_char_1 = re.compile(r'([\\W])')\n",
        "    parts = pattern_char_1.split(sent)\n",
        "    parts = [p for p in parts if len(p.strip())>0]\n",
        "    # 分割中文\n",
        "    pattern = re.compile(r'([\\u4e00-\\u9fa5])')\n",
        "    chars = pattern.split(sent)\n",
        "    chars = [w for w in chars if len(w.strip())>0]\n",
        "    return chars\n",
        "\n",
        "def loadfile():\n",
        "    happy = []\n",
        "    angry = []\n",
        "    disgust=[]\n",
        "    sad=[]\n",
        "    with open('/content/drive/MyDrive/NLP/data/happy_clean.txt',encoding='UTF-8') as f:\n",
        "        for line in f.readlines():\n",
        "            #happy.append(seg_char((line)))\n",
        "            happy.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
        "    with open('/content/drive/MyDrive/NLP/data/angry_clean.txt',encoding='UTF-8') as f:\n",
        "        for line in f.readlines():\n",
        "            #angry.append(seg_char((line)))\n",
        "            angry.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
        "        f.close()\n",
        "    with open('/content/drive/MyDrive/NLP/data/disgust_clean.txt',encoding='UTF-8') as f:\n",
        "        for line in f.readlines():\n",
        "            #disgust.append(seg_char((line)))\n",
        "            disgust.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
        "        f.close()\n",
        "    with open('/content/drive/MyDrive/NLP/data/sad_clean.txt',encoding='UTF-8') as f:\n",
        "        for line in f.readlines():\n",
        "            #sad.append(seg_char((line)))\n",
        "            sad.append(list(jieba.cut(line, cut_all=False, HMM=True))[:-1])\n",
        "        f.close()\n",
        "\n",
        "    X_Vec = np.concatenate((happy,angry,disgust,sad))\n",
        "\n",
        "    y = np.concatenate((np.zeros(len(happy), dtype=int),\n",
        "                        np.ones(len(angry), dtype=int),\n",
        "                        2*np.ones(len(disgust), dtype=int),\n",
        "                        3*np.ones(len(sad), dtype=int)))\n",
        "\n",
        "    return X_Vec, y"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD9CP-ZGZWQb"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfuUIMgNZqt0"
      },
      "source": [
        "#Word2Vec\n",
        "voc_dim = 150 #word的向量维度\n",
        "min_out = 4  #单词出现频率数\n",
        "window_size = 7 #\n",
        "cpu_count = multiprocessing.cpu_count()\n",
        "\n",
        "def word2vec_train(X_Vec):\n",
        "    model_word = Word2Vec(size=voc_dim,#vector_size=voc_dim\n",
        "                min_count=min_out,\n",
        "                window=window_size,\n",
        "                workers=cpu_count,\n",
        "                iter=100) # an empty model, no training yet\n",
        "    model_word.build_vocab(X_Vec)\n",
        "    model_word.train(X_Vec, total_examples=model_word.corpus_count, epochs=model_word.epochs)\n",
        "    model_word.save('/content/drive/MyDrive/NLP/model/Word2Vec_java.pkl')\n",
        "\n",
        "    print(len(model_word.wv.vocab))\n",
        "    input_dim = len(model_word.wv.vocab.keys()) + 1 #频数小于阈值的词语统统放一起，编码为0\n",
        "    embedding_weights = np.zeros((input_dim, voc_dim))\n",
        "    w2dic={}\n",
        "    for i in range(len(model_word.wv.vocab.keys())):\n",
        "        #embedding_weights[i+1, :] = model_word[list(model_word.wv.vocab.keys())[i]]\n",
        "        embedding_weights[i + 1, :] = model_word.wv[list(model_word.wv.vocab.keys())[i]]\n",
        "        w2dic[list(model_word.wv.vocab.keys())[i]]=i+1\n",
        "    return input_dim,embedding_weights,w2dic"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jPsZ_rnZlmY"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwJCi3LpYg4m"
      },
      "source": [
        "#lstm\n",
        "lstm_input = 150#lstm输入维度\n",
        "voc_dim = 150 #word的向量维度\n",
        "def lstm(input_dim, embedding_weights):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(output_dim=voc_dim,\n",
        "                        input_dim=input_dim,\n",
        "                        mask_zero=True,\n",
        "                        weights=[embedding_weights],\n",
        "                        input_length=lstm_input))\n",
        "    model.add(LSTM(256, activation='softsign'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWX8WwiYYn6k"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxKeE0jJVmMo",
        "outputId": "18e3e60c-c299-4b68-cead-ef44ace10e99"
      },
      "source": [
        "#Main\n",
        "imp.reload(sys)\n",
        "np.random.seed()\n",
        "#参数\n",
        "\n",
        "voc_dim = 150 #word的向量维度\n",
        "lstm_input = 150#lstm输入维度\n",
        "epoch_time = 10#epoch\n",
        "batch_size = 32 #batch\n",
        "\n",
        "def data2inx(w2indx,X_Vec):\n",
        "    data = []\n",
        "    for sentence in X_Vec:\n",
        "        new_txt = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                new_txt.append(w2indx[word])\n",
        "            except:\n",
        "                new_txt.append(0)\n",
        "\n",
        "        data.append(new_txt)\n",
        "    return data\n",
        "def train_lstm(model, x_train, y_train, x_test, y_test):\n",
        "    print('Compiling the Model...')\n",
        "    model.compile(loss='binary_crossentropy',#hinge\n",
        "                  optimizer='adam', metrics=['mae', 'acc'])\n",
        "\n",
        "    print(\"Train...\" )\n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epoch_time, verbose=1)\n",
        "\n",
        "    print(\"Evaluate...\")\n",
        "    print(model.predict(x_test))\n",
        "    score = model.evaluate(x_test, y_test,\n",
        "                           batch_size=batch_size)\n",
        "\n",
        "    yaml_string = model.to_yaml()\n",
        "    with open('/content/drive/MyDrive/NLP/model/lstm_java.yml', 'w') as outfile:\n",
        "        outfile.write(yaml.dump(yaml_string, default_flow_style=True))\n",
        "    model.save('/content/drive/MyDrive/NLP/model/lstm_java_total.h5')\n",
        "    print('Test score:', score)\n",
        "\n",
        "print(\"开始清洗数据................\")\n",
        "clean_data('/content/drive/MyDrive/NLP/data/happy.txt','/content/drive/MyDrive/NLP/data/happy_clean.txt')\n",
        "clean_data('/content/drive/MyDrive/NLP/data/angry.txt','/content/drive/MyDrive/NLP/data/angry_clean.txt')\n",
        "clean_data('/content/drive/MyDrive/NLP/data/disgust.txt','/content/drive/MyDrive/NLP/data/disgust_clean.txt')\n",
        "clean_data('/content/drive/MyDrive/NLP/data/sad.txt','/content/drive/MyDrive/NLP/data/sad_clean.txt')\n",
        "print(\"清洗数据完成................\")\n",
        "print(\"开始下载数据................\")\n",
        "X_Vec, y=loadfile()\n",
        "print(\"下载数据完成................\")\n",
        "print(\"开始构建词向量................\")\n",
        "input_dim,embedding_weights,w2dic = word2vec_train(X_Vec)\n",
        "print(\"构建词向量完成................\")\n",
        "\n",
        "index = data2inx(w2dic,X_Vec)\n",
        "index2 = sequence.pad_sequences(index, maxlen=voc_dim )\n",
        "x_train, x_test, y_train, y_test = train_test_split(index2, y, test_size=0.2)\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=4)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=4)\n",
        "\n",
        "model=lstm(input_dim, embedding_weights)\n",
        "train_lstm(model, x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始清洗数据................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "清洗数据完成................\n",
            "开始下载数据................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.017 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "下载数据完成................\n",
            "开始构建词向量................\n",
            "26913\n",
            "构建词向量完成................\n",
            "Compiling the Model...\n",
            "Train...\n",
            "Epoch 1/10\n",
            "998/998 [==============================] - 700s 698ms/step - loss: 0.5592 - mae: 0.3585 - acc: 0.3235\n",
            "Epoch 2/10\n",
            "998/998 [==============================] - 695s 697ms/step - loss: 0.5003 - mae: 0.3183 - acc: 0.4354\n",
            "Epoch 3/10\n",
            "998/998 [==============================] - 693s 694ms/step - loss: 0.4454 - mae: 0.2778 - acc: 0.5108\n",
            "Epoch 4/10\n",
            "998/998 [==============================] - 681s 683ms/step - loss: 0.3766 - mae: 0.2321 - acc: 0.5846\n",
            "Epoch 5/10\n",
            "998/998 [==============================] - 684s 685ms/step - loss: 0.3129 - mae: 0.1941 - acc: 0.6497\n",
            "Epoch 6/10\n",
            "998/998 [==============================] - 693s 694ms/step - loss: 0.2670 - mae: 0.1688 - acc: 0.6927\n",
            "Epoch 7/10\n",
            "998/998 [==============================] - 678s 679ms/step - loss: 0.2379 - mae: 0.1533 - acc: 0.7126\n",
            "Epoch 8/10\n",
            "998/998 [==============================] - 682s 683ms/step - loss: 0.2209 - mae: 0.1454 - acc: 0.7171\n",
            "Epoch 9/10\n",
            "998/998 [==============================] - 676s 677ms/step - loss: 0.2080 - mae: 0.1382 - acc: 0.7329\n",
            "Epoch 10/10\n",
            "998/998 [==============================] - 672s 673ms/step - loss: 0.2037 - mae: 0.1371 - acc: 0.7340\n",
            "Evaluate...\n",
            "[[1.18651202e-04 1.38180249e-03 4.15564835e-01 5.82934678e-01]\n",
            " [9.82351720e-01 1.74352303e-02 1.12893962e-04 1.00169236e-04]\n",
            " [1.59712341e-02 1.47618901e-03 5.18108845e-01 4.64443684e-01]\n",
            " ...\n",
            " [2.05134958e-01 7.66711831e-01 1.31346807e-02 1.50185125e-02]\n",
            " [9.99999285e-01 3.29394140e-07 2.39171072e-07 1.61496985e-07]\n",
            " [9.99997139e-01 2.60715933e-06 7.01680349e-08 6.26131040e-08]]\n",
            "250/250 [==============================] - 42s 166ms/step - loss: 0.7981 - mae: 0.2695 - acc: 0.4136\n",
            "Test score: [0.79813551902771, 0.26951152086257935, 0.41360732913017273]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}